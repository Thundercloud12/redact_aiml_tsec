{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7bab0f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._inductor' from 'C:\\Users\\Satyam\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_inductor\\__init__.py' has no attribute 'custom_graph_pass' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, WeightedRandomSampler\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mT\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# User config (defaults)\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01malexnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdensenet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mefficientnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\convnext.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstochastic_depth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_presets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\ops\\__init__.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgiou_loss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generalized_box_iou_loss\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpoolers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mps_roi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_align, PSRoIAlign\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mps_roi_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_pool, PSRoIPool\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\ops\\poolers.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mboxes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_area\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mroi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roi_align\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# is not supported by ONNX tracing yet.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# that merges the levels to the right indices\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;129m@torch\u001b[39m.jit.unused\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_onnx_merge_levels\u001b[39m(levels: Tensor, unmerged_results: List[Tensor]) -> Tensor:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\ops\\roi_align.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_compile_supported\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mannotations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BroadcastingList2\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pair\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\convert_frame.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\symbolic_convert.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\exc.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_dynamo\\utils.py:68\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal, TypeIs\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_functorch\\config.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Applies CSE to the graph before partitioning\u001b[39;00m\n\u001b[32m     38\u001b[39m cse = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fbcode\n\u001b[32m     43\u001b[39m enable_autograd_cache: \u001b[38;5;28mbool\u001b[39m = Config(\n\u001b[32m     44\u001b[39m     justknob=\u001b[33m\"\u001b[39m\u001b[33mpytorch/remote_cache:enable_local_autograd_cache\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     env_name_force=\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_AUTOGRAD_CACHE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     default=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     47\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremote_autograd_cache_default\u001b[39m() -> Optional[\u001b[38;5;28mbool\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_inductor\\__init__.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, IO, Optional, TYPE_CHECKING, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_inductor\\config.py:202\u001b[39m\n\u001b[32m    195\u001b[39m b2b_gemm_pass = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# register custom graph optimization pass hook. so far, pre/post passes are\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# only applied before/after pattern_matcher in post_grad_passes.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Implement CustomGraphPass to allow Inductor to graph compiled artifacts\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# to which your custom passes have been applied:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m post_grad_custom_pre_pass: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_inductor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_graph_pass\u001b[49m.CustomGraphPassType = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    203\u001b[39m post_grad_custom_post_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Registers a custom joint graph pass.\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch._inductor' from 'C:\\Users\\Satyam\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_inductor\\__init__.py' has no attribute 'custom_graph_pass' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# agriguard_pytorch.py\n",
    "# PyTorch version of AGRIGUARD training pipeline\n",
    "# Compatible with torch 2.x + CUDA (works with your torch 2.7.1+cu118)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, balanced_accuracy_score, classification_report,\n",
    "                             confusion_matrix, roc_auc_score)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "# ---------------------------\n",
    "# User config (defaults)\n",
    "# ---------------------------\n",
    "def get_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--data_dir\", default=\"data\", help=\"root data folder; expects train/<class> folders and optionally val/\")\n",
    "    p.add_argument(\"--img_size\", type=int, default=224)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    p.add_argument(\"--epochs_head\", type=int, default=8)\n",
    "    p.add_argument(\"--epochs_finetune\", type=int, default=12)\n",
    "    p.add_argument(\"--lr_head\", type=float, default=1e-3)\n",
    "    p.add_argument(\"--lr_finetune\", type=float, default=5e-5)\n",
    "    p.add_argument(\"--unfreeze_last_n\", type=int, default=30)\n",
    "    p.add_argument(\"--use_focal\", action=\"store_true\")\n",
    "    p.add_argument(\"--focal_alpha\", type=float, default=0.75)\n",
    "    p.add_argument(\"--focal_gamma\", type=float, default=2.0)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    p.add_argument(\"--save_path\", default=\"mobilenetv3_best.pt\")\n",
    "    p.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return p.parse_args()\n",
    "\n",
    "args = get_args()\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "DEVICE = torch.device(args.device)\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset & helpers\n",
    "# ---------------------------\n",
    "def discover_images(folder):\n",
    "    image_paths, labels = [], []\n",
    "    classes = sorted([d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d)) and not d.startswith(\".\")])\n",
    "    for idx, cls in enumerate(classes):\n",
    "        class_dir = os.path.join(folder, cls)\n",
    "        for p in glob(os.path.join(class_dir, \"*\")):\n",
    "            if p.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(p)\n",
    "                labels.append(idx)\n",
    "    return image_paths, labels, classes\n",
    "\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, df, img_size=224, training=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_size = img_size\n",
    "        self.training = training\n",
    "\n",
    "        # Transforms: GPU-friendly, deterministic-ish\n",
    "        if training:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((self.img_size, self.img_size)),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomVerticalFlip(),\n",
    "                T.RandomRotation(8),\n",
    "                T.RandomResizedCrop(self.img_size, scale=(0.9, 1.0)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((self.img_size, self.img_size)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row['path']\n",
    "        label = row['binary_label']  # 1 = healthy, 0 = diseased\n",
    "\n",
    "        # read image\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, torch.tensor(label, dtype=torch.float32), path\n",
    "\n",
    "# ---------------------------\n",
    "# Build dataframe (train/test split)\n",
    "# ---------------------------\n",
    "train_root = os.path.join(args.data_dir, \"train\")\n",
    "if not os.path.exists(train_root):\n",
    "    raise FileNotFoundError(f\"Expected {train_root} to exist and contain class subfolders\")\n",
    "\n",
    "image_paths, labels, classes = discover_images(train_root)\n",
    "print(f\"Found classes: {classes}\")\n",
    "print(\"Total images:\", len(image_paths))\n",
    "\n",
    "df = pd.DataFrame({\"path\": image_paths, \"class_idx\": labels})\n",
    "df['class_name'] = df['class_idx'].map(lambda i: classes[i])\n",
    "df['is_healthy'] = df['class_name'].str.contains('healthy', case=False, na=False)\n",
    "df['binary_label'] = df['is_healthy'].astype(int)\n",
    "\n",
    "total = len(df)\n",
    "healthy_count = df['binary_label'].sum()\n",
    "diseased_count = total - healthy_count\n",
    "print(f\"Healthy: {healthy_count}, Diseased: {diseased_count}\")\n",
    "\n",
    "# Stratified split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, stratify=df['binary_label'], random_state=args.seed)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['binary_label'], random_state=args.seed)\n",
    "print(\"Split sizes -> Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "\n",
    "# ---------------------------\n",
    "# Dataloaders\n",
    "# ---------------------------\n",
    "train_ds = PlantDataset(train_df, img_size=args.img_size, training=True)\n",
    "val_ds = PlantDataset(val_df, img_size=args.img_size, training=False)\n",
    "test_ds = PlantDataset(test_df, img_size=args.img_size, training=False)\n",
    "\n",
    "# We can optionally use a WeightedRandomSampler if class imbalance severe:\n",
    "counts = train_df['binary_label'].value_counts().to_dict()\n",
    "print(\"Train class counts:\", counts)\n",
    "class_sample_count = np.array([counts.get(0,0), counts.get(1,0)])\n",
    "# weight per sample: inverse of class frequency\n",
    "weights = 1.0 / class_sample_count[train_df['binary_label'].values]\n",
    "sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=args.batch_size, sampler=sampler,\n",
    "                          num_workers=args.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False,\n",
    "                        num_workers=args.num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False,\n",
    "                         num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Model, loss, optimizer\n",
    "# ---------------------------\n",
    "def build_model(pretrained=True):\n",
    "    # torchvision has mobilenet_v3_small\n",
    "    model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "    # replace classifier / head\n",
    "    in_features = model.classifier[0].in_features if hasattr(model, \"classifier\") else 576\n",
    "    # build a small head similar to TF variant\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d(1),  # some versions already do pooling; we keep safe\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 1)  # logits for BCEWithLogitsLoss\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(pretrained=True)\n",
    "model.to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# Freeze backbone (everything except final head) initially\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# compute pos_weight for BCEWithLogitsLoss (pos_weight = negative/positive)\n",
    "neg = counts.get(0, 0)\n",
    "pos = counts.get(1, 0)\n",
    "if pos == 0:\n",
    "    pos = 1  # avoid div by zero\n",
    "pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(DEVICE)\n",
    "print(\"pos_weight for BCEWithLogitsLoss:\", pos_weight.item())\n",
    "\n",
    "# Loss\n",
    "def focal_loss_with_logits(logits, targets, alpha=0.75, gamma=2.0, reduction=\"mean\"):\n",
    "    # logits: raw outputs (not sigmoid)\n",
    "    targets = targets.view(-1,1)\n",
    "    prob = torch.sigmoid(logits)\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    alpha_factor = targets * alpha + (1 - targets) * (1 - alpha)\n",
    "    focal_weight = alpha_factor * (1 - p_t) ** gamma\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "    loss = focal_weight * bce\n",
    "    return loss.mean() if reduction==\"mean\" else loss.sum()\n",
    "\n",
    "# We'll choose criterion at runtime\n",
    "if args.use_focal:\n",
    "    criterion = None  # we will call focal_loss manually\n",
    "    print(\"Using focal loss (custom)\")\n",
    "else:\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # handles imbalance\n",
    "    print(\"Using BCEWithLogitsLoss with pos_weight\")\n",
    "\n",
    "# Optimizers / schedulers\n",
    "def make_optimizer(model, lr):\n",
    "    # only params with requires_grad True\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "# Scheduler: linear warmup then cosine decay (per-epoch)\n",
    "def make_scheduler(optimizer, total_epochs, warmup_epochs):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch + 1) / float(max(1, warmup_epochs))\n",
    "        # cosine from 1 -> 0 over remaining epochs\n",
    "        t = (epoch - warmup_epochs) / max(1, (total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# ---------------------------\n",
    "# Training / Evaluation loops\n",
    "# ---------------------------\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, epoch, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    for imgs, labels, _ in loader:\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True).unsqueeze(1)  # shape (B,1)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "            logits = model(imgs)\n",
    "            if args.use_focal:\n",
    "                loss = focal_loss_with_logits(logits, labels, alpha=args.focal_alpha, gamma=args.focal_gamma)\n",
    "            else:\n",
    "                loss = criterion(logits, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        n += imgs.size(0)\n",
    "    return running_loss / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_paths = []\n",
    "    for imgs, labels, paths in loader:\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True).unsqueeze(1)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "            logits = model(imgs)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        all_labels.extend(labels.cpu().numpy().ravel().tolist())\n",
    "        all_probs.extend(probs.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_paths.extend(paths)\n",
    "    # metrics\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    y_prob = np.array(all_probs)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except:\n",
    "        auc = float('nan')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return {\n",
    "        'accuracy': acc, 'balanced_acc': bal, 'precision': prec,\n",
    "        'recall': rec, 'f1': f1, 'auc': auc, 'confusion_matrix': cm,\n",
    "        'y_true': y_true, 'y_pred': y_pred, 'y_prob': y_prob, 'paths': all_paths\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Full training orchestration\n",
    "# ---------------------------\n",
    "best_val_loss = float('inf')\n",
    "best_metric = None\n",
    "\n",
    "# HEAD training\n",
    "optimizer = make_optimizer(model, lr=args.lr_head)\n",
    "scheduler = make_scheduler(optimizer, total_epochs=args.epochs_head + args.epochs_finetune, warmup_epochs=2)\n",
    "\n",
    "print(\"Starting head training...\")\n",
    "for epoch in range(args.epochs_head):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, epoch, args.epochs_head + args.epochs_finetune)\n",
    "    scheduler.step()\n",
    "    val_stats = evaluate(model, val_loader)\n",
    "    val_loss = 1.0 - val_stats['f1']  # cheap proxy for saved metric (you can compute val loss too)\n",
    "    print(f\"Epoch {epoch+1}/{args.epochs_head} - train_loss: {loss:.4f}, val_f1: {val_stats['f1']:.4f}, val_acc: {val_stats['accuracy']:.4f}\")\n",
    "    # save best by f1\n",
    "    if val_stats['f1'] > (best_metric or 0):\n",
    "        best_metric = val_stats['f1']\n",
    "        torch.save({'model_state_dict': model.state_dict(), 'args': vars(args)}, args.save_path)\n",
    "        print(\"Saved best model ->\", args.save_path)\n",
    "\n",
    "# Fine-tune: unfreeze last N layers of backbone\n",
    "print(\"Unfreezing last\", args.unfreeze_last_n, \"layers of backbone for fine-tuning...\")\n",
    "# find backbone params and unfreeze last N by order\n",
    "backbone_layers = [n for n, p in model.named_parameters() if \"classifier\" not in n]\n",
    "# Unfreeze last N (best effort)\n",
    "for name, p in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        p.requires_grad = False\n",
    "# Attempt to unfreeze last N from backbone by iterating reversed order:\n",
    "count_unf = 0\n",
    "for name, p in reversed(list(model.named_parameters())):\n",
    "    if \"classifier\" in name:\n",
    "        continue\n",
    "    if count_unf < args.unfreeze_last_n:\n",
    "        p.requires_grad = True\n",
    "        count_unf += 1\n",
    "\n",
    "print(\"Trainable params after unfreeze:\")\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_p = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{trainable} / {total_p} trainable parameters\")\n",
    "\n",
    "# Recreate optimizer with lower LR\n",
    "optimizer = make_optimizer(model, lr=args.lr_finetune)\n",
    "scheduler = make_scheduler(optimizer, total_epochs=args.epochs_head + args.epochs_finetune, warmup_epochs=2)\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "for epoch in range(args.epochs_head, args.epochs_head + args.epochs_finetune):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, epoch, args.epochs_head + args.epochs_finetune)\n",
    "    scheduler.step()\n",
    "    val_stats = evaluate(model, val_loader)\n",
    "    print(f\"Fine Epoch {epoch+1}/{args.epochs_head+args.epochs_finetune} - train_loss: {loss:.4f}, val_f1: {val_stats['f1']:.4f}, val_acc: {val_stats['accuracy']:.4f}\")\n",
    "    if val_stats['f1'] > (best_metric or 0):\n",
    "        best_metric = val_stats['f1']\n",
    "        torch.save({'model_state_dict': model.state_dict(), 'args': vars(args)}, args.save_path)\n",
    "        print(\"Saved best model ->\", args.save_path)\n",
    "\n",
    "# Save final\n",
    "final_path = \"mobilenetv3_final.pt\"\n",
    "torch.save({'model_state_dict': model.state_dict(), 'args': vars(args)}, final_path)\n",
    "print(\"Saved final model ->\", final_path)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_stats = evaluate(model, test_loader)\n",
    "print(\"=== TEST RESULTS ===\")\n",
    "print(\"Accuracy:\", test_stats['accuracy'])\n",
    "print(\"Balanced Acc:\", test_stats['balanced_acc'])\n",
    "print(\"Precision:\", test_stats['precision'])\n",
    "print(\"Recall:\", test_stats['recall'])\n",
    "print(\"F1:\", test_stats['f1'])\n",
    "print(\"AUC:\", test_stats['auc'])\n",
    "print(\"Confusion matrix:\\n\", test_stats['confusion_matrix'])\n",
    "\n",
    "# Save predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    'path': test_stats['paths'],\n",
    "    'true': test_stats['y_true'],\n",
    "    'pred': test_stats['y_pred'],\n",
    "    'prob': test_stats['y_prob']\n",
    "})\n",
    "pred_df.to_csv(\"pytorch_test_predictions.csv\", index=False)\n",
    "print(\"Saved predictions -> pytorch_test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6754a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
